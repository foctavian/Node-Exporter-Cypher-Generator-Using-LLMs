from langchain_mistralai import ChatMistralAI
from pydantic import BaseModel, Field
from typing import List, TypedDict
from dataclasses import dataclass
from dotenv import load_dotenv
import os
import asyncio
import time
from langgraph.graph import END, StateGraph, START

class cypher(BaseModel):
    problem: str = Field(description="problem to solve")
    cypher_script: str = Field(description="cypher script to run")

@dataclass
class SystemCypherScript(TypedDict):  # this class is used to store the cypher scripts generated by the agent
    nodes: str = ""
    relationships: str = ""
    properties: str = ""
 
class Node(BaseModel):
    name: str = Field(description="name or id of the node")
    label: str = Field(description="label of the node, e.g. CPU, GPU, etc.")
    system: str = Field(description="system to which the node belongs")

class GraphState(TypedDict):
    error: str
    messages: List
    generation: str
    script: SystemCypherScript

class InferredNodes(BaseModel):
    problem: str = Field(description="problem to solve")
    nodes: List[Node] = Field(description="list of nodes to create")

load_dotenv()
mistral_model = "codestral-latest"
llm = ChatMistralAI(model=mistral_model,
                    temperature=0,
                    max_tokens=20000,
                    api_key=os.getenv('MISTRAL_API_KEY'))
structured_llm = llm.with_structured_output(InferredNodes)

def clean_exporter_file(file):
    new_lines = []
    with open(file, 'r') as f:
        lines = f.readlines()
        for line in lines:
            if line.startswith('#'):
                continue
            new_lines.append(line)
    return '\n'.join(new_lines)

def parse_file(filename):
    return clean_exporter_file(filename)

def chunk_metrics(metrics: str):
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=6000,
        chunk_overlap=100,
        length_function=len,
        is_separator_regex=False,
    )
    chunks = text_splitter.split_text(metrics)
    return chunks

async def infer_nodes(state: GraphState):
    chunks = chunk_metrics(state['generation'])
    all_messages = []
    all_responses = []
    
    for chunk in chunks:
        prompt = f"""
        You are a graph agent. Your task is to infer the nodes from the following node exporter metrics. The format of the nodes is similar to Neo4j.
        These are the metrics:
        {chunk}
        """
        all_messages.append(prompt)
    
    # Process in smaller batches to avoid rate limiting
    batch_size = 2  # Adjust based on Mistral's rate limits
    start_time = time.time()
    
    for i in range(0, len(all_messages), batch_size):
        batch = all_messages[i:i+batch_size]
        try:
            batch_responses = await llm.abatch(batch)
            all_responses.extend(batch_responses)
            # Add a delay between batches to avoid rate limiting
            if i + batch_size < len(all_messages):
                await asyncio.sleep(1)  # 1 second delay between batches
        except Exception as e:
            print(f"Error processing batch {i//batch_size}: {e}")
            # Add exponential backoff if needed
            await asyncio.sleep(5)
            # Retry with smaller batch or individually
            for msg in batch:
                try:
                    response = await llm.ainvoke(msg)
                    all_responses.append(response)
                    await asyncio.sleep(0.5)  # Short delay between individual requests
                except Exception as inner_e:
                    print(f"Failed individual request: {inner_e}")
    
    end_time = time.time()
    print(f"Batch processing took {end_time - start_time:.2f} seconds.")
    
    for response in all_responses:
        print(response)
    
    return state  # Make sure to return the state

def init_graph():
    workflow = StateGraph(GraphState)
    workflow.add_node("infer_nodes", infer_nodes)
    workflow.add_edge(START, "infer_nodes")
    workflow.add_edge("infer_nodes", END)
    app = workflow.compile()
    return app

async def start_agent(filename='./uploads/cpu_intensive_worload_pc1.txt'):
    data = parse_file(filename)
    app = init_graph()
    empty_script = SystemCypherScript()
    await app.ainvoke({"messages": [], "error": "", 'script': empty_script, 'generation': data})

# Run only if this script is executed directly
if __name__ == "__main__":
    asyncio.run(start_agent())